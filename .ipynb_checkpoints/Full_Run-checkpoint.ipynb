{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ca1687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/traffic/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/traffic/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/traffic/.local/lib/python3.8/site-packages/gensim/matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import triu\n",
      "2022-05-12 20:25:10.071642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-12 20:25:10.071702: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import gensim\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "\n",
    "#for sentiment analysis\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "import sentiment as sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8160d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sentiment' from '/home/traffic/code/App_Sentiment_plot/sentiment.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a012d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## authenticate with BigQuery API\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../../datascience-abovezero-58d98dcf7f65.json')\n",
    "client = bigquery.Client.from_service_account_json(\n",
    "    '../../datascience-abovezero-58d98dcf7f65.json')\n",
    "# Perform a query.\n",
    "QUERY = ('SELECT * FROM `datascience-abovezero.ml_sandbox.chegg_influencers_comments`')\n",
    "query_job = client.query(QUERY)  # API request\n",
    "df =  query_job.result().to_dataframe() #transform to Pandas Dataframe\n",
    "\n",
    "#df2 = pd.read_csv('analisis_comments_tiktok.csv')# Parameters tuning using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecafea",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6d01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### settings for topic analysis\n",
    "\n",
    "N_COMPONENTS = 4 #number of topics\n",
    "\n",
    "\n",
    "\n",
    "### settings for sentiment analysis\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 1\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 32\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 60\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22777aeb",
   "metadata": {},
   "source": [
    "# 1. Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e726f",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c9fa1",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6bef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def spelling_correcter(text):\n",
    "    \n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n",
    "comments = df.comment.tolist()\n",
    "comments_prep = [preprocess(str(comment)) for comment in comments]\n",
    "comments_dict = gensim.corpora.Dictionary(comments_prep)\n",
    "bow_corpus = [comments_dict.doc2bow(doc) for doc in comments_prep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff327ed2",
   "metadata": {},
   "source": [
    "## TFIDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef3e42",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4e2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### docs_raw = df.comment.fillna('').tolist()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 0.01)\n",
    "#dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "#dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47632af9",
   "metadata": {},
   "source": [
    "### Training The Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9ca91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/traffic/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['school', 'thank', 'quot', 'like', 'high'], 1: ['cancer', 'colon', 'youtube', 'amp', 'literally'], 2: ['love', 'cancer', 'simone', 'colon', 'year'], 3: ['video', 'life', 'just', 'like', 'time']}\n"
     ]
    }
   ],
   "source": [
    "def get_top_topics(model, vectorizer, topn=5):\n",
    "    names = tf_vectorizer.get_feature_names()\n",
    "    res = {}\n",
    "    for i_cluster, cluster in enumerate(model.components_):\n",
    "        res[i_cluster]=[]\n",
    "        for i_feature in cluster.argsort()[:-topn - 1:-1]:\n",
    "            res[i_cluster] = res[i_cluster] + [names[i_feature]]\n",
    "    return res\n",
    "\n",
    "## first get topics for all comments (Full Dataset)\n",
    "docs_raw_full = df.comment.fillna('').tolist()\n",
    "dtm_tf_full = tf_vectorizer.fit_transform(docs_raw_full)\n",
    "# train model to find topics per influencer\n",
    "lda_tf_full = LatentDirichletAllocation(n_components=N_COMPONENTS, random_state=0)\n",
    "lda_tf_full.fit(dtm_tf_full)\n",
    "# extract most important topics\n",
    "topics_full =  get_top_topics(lda_tf_full, tf_vectorizer, topn=5)\n",
    "print(topics_full)\n",
    "\n",
    "X_test = tf_vectorizer.transform(df.loc[:,'comment'])\n",
    "\n",
    "doc_topic_dist_unnormalized_full = np.matrix(lda_tf_full.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b50ab",
   "metadata": {},
   "source": [
    "### Scoring the data + arranging results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac90746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['school', 'thank', 'quot', 'like', 'high'], 1: ['cancer', 'colon', 'youtube', 'amp', 'literally'], 2: ['love', 'cancer', 'simone', 'colon', 'year'], 3: ['video', 'life', 'just', 'like', 'time']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6198/2131007174.py:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_6198/2131007174.py:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_6198/2131007174.py:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_6198/2131007174.py:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_6198/2131007174.py:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_6198/2131007174.py:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n"
     ]
    }
   ],
   "source": [
    "#first get topics for all comments (Full Dataset)\n",
    "docs_raw_full = df.comment.fillna('').tolist()\n",
    "dtm_tf_full = tf_vectorizer.fit_transform(docs_raw_full)\n",
    "# train model to find topics per influencer\n",
    "lda_tf_full = LatentDirichletAllocation(n_components=4, random_state=0)\n",
    "lda_tf_full.fit(dtm_tf_full)\n",
    "# extract most important topics\n",
    "topics_full =  get_top_topics(lda_tf_full, tf_vectorizer, topn=5)\n",
    "print(topics_full)\n",
    "\n",
    "#now let's what of those topics are talked about by what influencer\n",
    "# and then go more granular and look at topics on influencer level.\n",
    "topic_mapping = pd.DataFrame()\n",
    "res_infl = pd.DataFrame({'influencer':[]})\n",
    "for topic in topics_full:\n",
    "    topic_mapping.loc[:,topic] = np.array([str(topics_full[topic])])\n",
    "    res_infl[topic] = []\n",
    "    \n",
    "for topic in topics_full:\n",
    "    res_infl['likesrel_'+str(topic)] = []\n",
    "for topic in topics_full:\n",
    "    res_infl['likestot_'+str(topic)] = []\n",
    "\n",
    "for i_infl, infl in enumerate(df.influencer.unique()):\n",
    "    dfi = df.loc[df.influencer==infl,:]\n",
    "    #what categories of full topic model do infl comments belong to\n",
    "    X_test = tf_vectorizer.transform(dfi.loc[:,'comment'])\n",
    "    doc_topic_dist_unnormalized_full = np.matrix(lda_tf_full.transform(X_test))\n",
    "    # get count of number topics are 'hit'\n",
    "    res_ = pd.DataFrame({'topic':list(topics_full.keys())})\n",
    "    \n",
    "    res__ = pd.DataFrame(doc_topic_dist_unnormalized_full.argmax(axis=1)).value_counts().rename_axis('topic').reset_index(name='counts')\n",
    "    res_ = res_.merge(res__, on='topic', how='left').reset_index(drop=True).fillna(0)\n",
    "    \n",
    "    # calculate relative topic distribution for influencer\n",
    "    counts_sum = res_.counts.sum()\n",
    "    res_.loc[:,'counts'] = res_.counts.apply(lambda x:round(x/counts_sum*100,1))\n",
    "    res_ = res_.sort_values('topic',ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    l_likecount_tot = len(topics_full)*[0]\n",
    "    total_likecount = dfi.comment_likecount.sum()\n",
    "    for comment, comment_orig, comment_likes in zip(doc_topic_dist_unnormalized_full, dfi.comment, dfi.comment_likecount):\n",
    "        l_likecount_tot[comment[0].argmax()]+=comment_likes\n",
    "    l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
    "    res_infl.loc[len(res_infl)] = [infl] + list(res_.counts) + l_likecount_rel + l_likecount_tot\n",
    " \n",
    "    \"\"\"  \n",
    "    docs_raw = dfi.comment.fillna('').tolist()\n",
    "    dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
    "    # train model to find topics per influencer\n",
    "    lda_tf = LatentDirichletAllocation(n_components=4, random_state=0)\n",
    "    lda_tf.fit(dtm_tf)d\n",
    "    # extract most important topics\n",
    "    topics =  get_top_topics(lda_tf, tf_vectorizer, topn=5)\n",
    "    print(infl)\n",
    "    print(topics)\n",
    "    \n",
    "    doc_topic_dist_unnormalized = np.matrix(lda_tf.transform(X_test))\n",
    "    for comment, comment_orig in zip(doc_topic_dist_unnormalized, dfi.comment):\n",
    "        print(comment_orig)\n",
    "        print(topics[comment[0].argmax()])\n",
    "    \"\"\"\n",
    "    #if i_infl>-1:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59bea655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>influencer</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>likesrel_0</th>\n",
       "      <th>likesrel_1</th>\n",
       "      <th>likesrel_2</th>\n",
       "      <th>likesrel_3</th>\n",
       "      <th>likestot_0</th>\n",
       "      <th>likestot_1</th>\n",
       "      <th>likestot_2</th>\n",
       "      <th>likestot_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>itssozer</td>\n",
       "      <td>59.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emilyballz</td>\n",
       "      <td>73.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iamalilstitious</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sarahbada_</td>\n",
       "      <td>73.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sakshammagic</td>\n",
       "      <td>73.1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        influencer     0     1     2     3  likesrel_0  likesrel_1  \\\n",
       "0         itssozer  59.2   8.2  14.3  18.4    0.722222    0.006944   \n",
       "1       emilyballz  73.7   5.3  15.8   5.3    0.789474    0.157895   \n",
       "2  iamalilstitious  50.0   0.0  37.5  12.5    0.666667    0.000000   \n",
       "3       sarahbada_  73.7   5.3  15.8   5.3    0.913043    0.000000   \n",
       "4     sakshammagic  73.1  11.5   3.8  11.5    0.761905    0.142857   \n",
       "\n",
       "   likesrel_2  likesrel_3  likestot_0  likestot_1  likestot_2  likestot_3  \n",
       "0    0.243056    0.027778         104           1          35           4  \n",
       "1    0.000000    0.052632          15           3           0           1  \n",
       "2    0.333333    0.000000           2           0           1           0  \n",
       "3    0.086957    0.000000          21           0           2           0  \n",
       "4    0.000000    0.095238          16           3           0           2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_infl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496929fe",
   "metadata": {},
   "source": [
    "# 2. Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf1339",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Model + creating Datasets for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424d6c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuraiton...\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cpu`\n",
      "Dealing with Training Data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee7d3912c7e4e75a43644146211c603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001eebdcbda141b0b7634a3a3de61970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 25000 examples!\n",
      "Created `train_dataloader` with 782 batches!\n",
      "\n",
      "Dealing with Validation Data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bea8f71dfdf4b2b8dcc84fe7ff0b181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63b884af16141f7ad75a72e2d117434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `valid_dataset` with 25000 examples!\n",
      "Created `eval_dataloader` with 782 batches!\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "global model\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = sent.Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "print('Dealing with Training Data...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = sent.MovieReviewsDataset(path='/home/traffic/data/aclImdb/train', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = sent.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation Data...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  sent.MovieReviewsDataset(path='/home/traffic/data/aclImdb/test', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = sent.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4cf2f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ad55257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sentiment' from '/home/traffic/code/App_Sentiment_plot/sentiment.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7de9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c985f24c90e4023b081265430a313ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cb902438be43cea0b269728bfa2cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
    "# us the number of batches.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print()\n",
    "  print('Training on batches...')\n",
    "  # Perform one full pass over the training set.\n",
    "  train_labels, train_predict, train_loss = sent.train(train_dataloader, optimizer, scheduler, device, model)\n",
    "  train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "  # Get prediction form model on validation data. \n",
    "  print('Validation on batches...')\n",
    "  valid_labels, valid_predict, val_loss = sent.validation(valid_dataloader, device, model)\n",
    "  val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "  # Print loss and accuracy values to see how training evolves.\n",
    "  print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "  print()\n",
    "\n",
    "  # Store the loss value for plotting the learning curve.\n",
    "  all_loss['train_loss'].append(train_loss)\n",
    "  all_loss['val_loss'].append(val_loss)\n",
    "  all_acc['train_acc'].append(train_acc)\n",
    "  all_acc['val_acc'].append(val_acc)\n",
    "\n",
    "# Plot loss curves.\n",
    "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
    "\n",
    "# Plot accuracy curves.\n",
    "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdabfce",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction form model on validation data. This is where you should use\n",
    "# your test data.\n",
    "true_labels, predictions_labels, avg_epoch_loss = sent.validation(valid_dataloader, device, model)\n",
    "\n",
    "# Create the evaluation report.\n",
    "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
    "# Show the evaluation report.\n",
    "print(evaluation_report)\n",
    "\n",
    "# Plot confusion matrix.\n",
    "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels, \n",
    "                      classes=list(labels_ids.keys()), normalize=True, \n",
    "                      magnify=0.1,\n",
    "                      );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9104405",
   "metadata": {},
   "source": [
    "## Applying Model to our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('analisis_comments_tiktok.csv')\n",
    "comments = [str(comment) for comment in df.comment.fillna('').tolist()]\n",
    "chegg_dataset = CheggDataset(df=comments, use_tokenizer=tokenizer)\n",
    "\n",
    "chegg_dataloader = DataLoader(chegg_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "\n",
    "_, predictions_labels, _ = validation(chegg_dataloader, device)\n",
    "\n",
    "results = pd.DataFrame({'comments':comments,'labels':predictions_labels})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f98e2",
   "metadata": {},
   "source": [
    "## Write Results to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3883570",
   "metadata": {},
   "outputs": [],
   "source": [
    "## def save_results(df, table_id=\"datascience-abovezero.ml_sandbox.chegg_influencers_comments_results\"):\n",
    "    # create BigQuery table if it doesn't already exist\n",
    "    try: \n",
    "        client.get_table(table_id)  # Make an API request.\n",
    "        print(\"Table {} already exists.\".format(table_id))\n",
    "    except NotFound:\n",
    "        print(f\"Table {table_id} created.\")\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"influencer\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('_'.join(['topic0'] + topics_full[0]), \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('_'.join(['topic1'] + topics_full[1]), \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('_'.join(['topic2'] + topics_full[2]), \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('_'.join(['topic3'] + topics_full[3]), \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likesrel0', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likesrel1', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likesrel2', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likesrel3', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likestot0', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likestot1', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likestot2', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likestot3', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "        ]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        #print(\n",
    "        #    \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        #)\n",
    "    df.columns = ['influencer', '_'.join(['topic']+topics_full[0]),'_'.join(['topic']+topics_full[1]),'_'.join(['topic']+topics_full[2]),'_'.join(['topic']+topics_full[3]),\n",
    "                 'likesrel0','likesrel1','likesrel2','likesrel3','likestot0','likestot1','likestot2','likestot3']     \n",
    "    df.to_gbq('.'.join(table_id.split('.')[1:]), project_id=table_id.split('.')[0], if_exists='replace',#'append',\n",
    "          chunksize=10000, progress_bar=True, credentials=credentials)\n",
    "    \n",
    "    \n",
    "save_results(res_infl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
