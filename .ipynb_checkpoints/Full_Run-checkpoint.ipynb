{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ca1687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/traffic/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/traffic/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/traffic/.local/lib/python3.8/site-packages/gensim/matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import triu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import gensim\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33cf4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate with BigQuery API\n",
    "client = bigquery.Client.from_service_account_json('../../datascience-abovezero-58d98dcf7f65.json')\n",
    "# Perform a query.\n",
    "QUERY = ('SELECT * FROM `datascience-abovezero.ml_sandbox.chegg_influencers_comments`')\n",
    "query_job = client.query(QUERY)  # API request\n",
    "df =  query_job.result().to_dataframe() #transform to Pandas Dataframe\n",
    "\n",
    "#df2 = pd.read_csv('analisis_comments_tiktok.csv')# Parameters tuning using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e726f",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c9fa1",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6bef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correcter(text):\n",
    "    \n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n",
    "comments = df.comment.tolist()\n",
    "comments_prep = [preprocess(str(comment)) for comment in comments]\n",
    "comments_dict = gensim.corpora.Dictionary(comments_prep)\n",
    "bow_corpus = [comments_dict.doc2bow(doc) for doc in comments_prep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2a8314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ayyyi\n",
      "1 legend\n",
      "2 call\n",
      "3 colleg\n",
      "4 univers\n",
      "5 amaz\n",
      "6 person\n",
      "7 antedragz\n",
      "8 chegg\n",
      "9 fuck\n",
      "10 gift\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in comments_dict.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa3f57",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cb1c402",
   "metadata": {},
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 4, \n",
    "                                   id2word = comments_dict,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04e73c",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc3aee6f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "unseen_document = 'I hate Chegg. What a failure!'# Data preprocessing step for the unseen document\n",
    "bow_vector = comments_dict.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(f\"Topic: {index} Score: {score}\\t, Topic detail:{lda_model.print_topic(index, 5)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5055149f",
   "metadata": {},
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda_model, comments_dict,bow_corpus,mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff327ed2",
   "metadata": {},
   "source": [
    "# TFIDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef3e42",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc7d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_raw = df.comment.fillna('').tolist()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 0.01)\n",
    "#dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "#dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47632af9",
   "metadata": {},
   "source": [
    "### Training The Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3601ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topics(model, vectorizer, topn=5):\n",
    "    names = tf_vectorizer.get_feature_names()\n",
    "    res = {}\n",
    "    for i_cluster, cluster in enumerate(model.components_):\n",
    "        res[i_cluster]=[]\n",
    "        for i_feature in cluster.argsort()[:-topn - 1:-1]:\n",
    "            res[i_cluster] = res[i_cluster] + [names[i_feature]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2fb106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itssozer\n",
      "{0: ['happy', 'best', 'nice', 'uni', 'makes'], 1: ['educate', 'antedragz', 'gift', 'like', 'itssozer'], 2: ['college', 'uni', 'person', 'know', 'different'], 3: ['uni', 'day', 'reply', 'does', 'university']}\n",
      "Ayyyy legend\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Are you serious\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Yes it's college but it's called university\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Your such an amazing person üòÅ\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "if i got chegg as a gift i would fucking die @Antedragz\n",
      "['educate', 'antedragz', 'gift', 'like', 'itssozer']\n",
      "That‚Äôs a different thing in my country tho\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "ÔºöCe gars est une legende üëè\n",
      "['educate', 'antedragz', 'gift', 'like', 'itssozer']\n",
      "Omg your so nice\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "What a gift\n",
      "['educate', 'antedragz', 'gift', 'like', 'itssozer']\n",
      "HEAVEN NOW\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "I love your page @itssozer ü•∞üòÅ\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Why would anyone ever hurt you? Your the nicest person I knowü•∞üòÅ\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "hii I have CRPS a very rare disease and my mum dose not have enough money to pay for my medical bills\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "Yes it‚Äôs university\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Why don‚Äôt you guys get some goods from the soulmia appü•∞\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "@Antedragz\n",
      "['educate', 'antedragz', 'gift', 'like', 'itssozer']\n",
      "The best\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "is this sponsored? idk what gave it awayüò≥\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "me wathching her tiktok:üò≥..\n",
      "mom:why you sad\n",
      "me:because my brother he broke my cellphone...\n",
      "@itssozer this not making story this is true...\n",
      "['educate', 'antedragz', 'gift', 'like', 'itssozer']\n",
      "uni means university and usually a uni will have lots of colleges under it that have different fields and majors you can study\n",
      "['uni', 'day', 'reply', 'does', 'university']\n",
      "this is the best acount i‚Äôve ever seen, it makes me be happy, you are the best broü•∞\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "sorry to educate myself\n",
      "['educate', 'antedragz', 'gift', 'like', 'itssozer']\n",
      "You are awesome nice kind and it makes me and everyone happy\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "This guy literally always makes my day\n",
      "['uni', 'day', 'reply', 'does', 'university']\n",
      "He‚Äôs in Adelaide\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "@jenna.michellle RAWR\n",
      "['uni', 'day', 'reply', 'does', 'university']\n",
      "üôå‚ù§Ô∏è\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Uni is so hard! so happy to see you're supporting some uni students\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "I‚Äôm just trying to educate him like he wanted ü§∑üèª‚Äç‚ôÄÔ∏è\n",
      "['educate', 'antedragz', 'gift', 'like', 'itssozer']\n",
      "Same here but I think this person is American\n",
      "['uni', 'day', 'reply', 'does', 'university']\n",
      "Lmao good for you? üòÇ\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "Okay so will I\n",
      "['uni', 'day', 'reply', 'does', 'university']\n",
      "You too üòÇ\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "I‚Äôm so glad I came across you ma‚Äôam @Aviana Dominic my financial debt is now a thing of the past. God bless you ma‚Äôam\n",
      "['educate', 'antedragz', 'gift', 'like', 'itssozer']\n",
      "Your a very kind person ü•∞\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "No problem üòÅ\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Can I get a hi ü•∞ your my favourite and first to like!\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "OMG\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Does the king reply? üëë\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "I‚Äôll do what i want thankssss\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "Uni is stresssssfffffuuuuulllllllll\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "thanks\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "Does the goat reply? Pls do it will make my day üòÖ\n",
      "['uni', 'day', 'reply', 'does', 'university']\n",
      "you so nice\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "Not in the UK; college & Uni are different things\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "Yeah I know but that person I think is from America and they call uni college\n",
      "['college', 'uni', 'person', 'know', 'different']\n",
      "14th\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "1m and hi\n",
      "['happy', 'best', 'nice', 'uni', 'makes']\n",
      "what's UNI? College?\n",
      "['college', 'uni', 'person', 'know', 'different']\n"
     ]
    }
   ],
   "source": [
    "for i_infl, infl in enumerate(df.influencer.unique()):\n",
    "    dfi = df.loc[df.influencer==infl,:]\n",
    "    docs_raw = dfi.comment.fillna('').tolist()\n",
    "    dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
    "    # train model to find topics\n",
    "    lda_tf = LatentDirichletAllocation(n_components=4, random_state=0)\n",
    "    lda_tf.fit(dtm_tf)\n",
    "    # extract most important topics\n",
    "    topics =  get_top_topics(lda_tf, tf_vectorizer, topn=5)\n",
    "    print(infl)\n",
    "    print(topics)\n",
    "    \n",
    "    X_test = tf_vectorizer.transform(dfi.loc[:,'comment'])\n",
    "    doc_topic_dist_unnormalized = np.matrix(lda_tf.transform(X_test))\n",
    "    for comment, comment_orig in zip(doc_topic_dist_unnormalized, dfi.comment):\n",
    "        print(comment_orig)\n",
    "        print(topics[comment[0].argmax()])\n",
    "    if i_infl>-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2ead95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_post</th>\n",
       "      <th>date_extraction</th>\n",
       "      <th>influencer</th>\n",
       "      <th>post_type</th>\n",
       "      <th>post_url</th>\n",
       "      <th>platform</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_likecount</th>\n",
       "      <th>replies</th>\n",
       "      <th>BU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-9-2</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>itssozer</td>\n",
       "      <td>VIDEO</td>\n",
       "      <td>https://www.tiktok.com/@itssozer/video/7003502...</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>Ayyyy legend</td>\n",
       "      <td>50</td>\n",
       "      <td>None</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-9-2</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>itssozer</td>\n",
       "      <td>VIDEO</td>\n",
       "      <td>https://www.tiktok.com/@itssozer/video/7003502...</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>Are you serious</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-9-2</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>itssozer</td>\n",
       "      <td>VIDEO</td>\n",
       "      <td>https://www.tiktok.com/@itssozer/video/7003502...</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>Yes it's college but it's called university</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-9-2</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>itssozer</td>\n",
       "      <td>VIDEO</td>\n",
       "      <td>https://www.tiktok.com/@itssozer/video/7003502...</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>Your such an amazing person üòÅ</td>\n",
       "      <td>33</td>\n",
       "      <td>None</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-9-2</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>itssozer</td>\n",
       "      <td>VIDEO</td>\n",
       "      <td>https://www.tiktok.com/@itssozer/video/7003502...</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>if i got chegg as a gift i would fucking die @...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_post date_extraction influencer post_type  \\\n",
       "0  2021-9-2      2021-12-01   itssozer     VIDEO   \n",
       "1  2021-9-2      2021-12-01   itssozer     VIDEO   \n",
       "2  2021-9-2      2021-12-01   itssozer     VIDEO   \n",
       "3  2021-9-2      2021-12-01   itssozer     VIDEO   \n",
       "4  2021-9-2      2021-12-01   itssozer     VIDEO   \n",
       "\n",
       "                                            post_url platform  \\\n",
       "0  https://www.tiktok.com/@itssozer/video/7003502...   TikTok   \n",
       "1  https://www.tiktok.com/@itssozer/video/7003502...   TikTok   \n",
       "2  https://www.tiktok.com/@itssozer/video/7003502...   TikTok   \n",
       "3  https://www.tiktok.com/@itssozer/video/7003502...   TikTok   \n",
       "4  https://www.tiktok.com/@itssozer/video/7003502...   TikTok   \n",
       "\n",
       "                                             comment  comment_likecount  \\\n",
       "0                                       Ayyyy legend                 50   \n",
       "1                                    Are you serious                  8   \n",
       "2        Yes it's college but it's called university                  9   \n",
       "3                      Your such an amazing person üòÅ                 33   \n",
       "4  if i got chegg as a gift i would fucking die @...                  0   \n",
       "\n",
       "  replies  BU  \n",
       "0    None  CS  \n",
       "1    None  CS  \n",
       "2    None  CS  \n",
       "3    None  CS  \n",
       "4    None  CS  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c4575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
