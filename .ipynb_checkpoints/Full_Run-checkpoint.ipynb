{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ca1687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/traffic/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/traffic/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import gensim\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33cf4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate with BigQuery API\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../../datascience-abovezero-58d98dcf7f65.json')\n",
    "client = bigquery.Client.from_service_account_json(\n",
    "    '../../datascience-abovezero-58d98dcf7f65.json')\n",
    "# Perform a query.\n",
    "QUERY = ('SELECT * FROM `datascience-abovezero.ml_sandbox.chegg_influencers_comments`')\n",
    "query_job = client.query(QUERY)  # API request\n",
    "df =  query_job.result().to_dataframe() #transform to Pandas Dataframe\n",
    "\n",
    "#df2 = pd.read_csv('analisis_comments_tiktok.csv')# Parameters tuning using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e726f",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c9fa1",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d6bef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def spelling_correcter(text):\n",
    "    \n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n",
    "comments = df.comment.tolist()\n",
    "comments_prep = [preprocess(str(comment)) for comment in comments]\n",
    "comments_dict = gensim.corpora.Dictionary(comments_prep)\n",
    "bow_corpus = [comments_dict.doc2bow(doc) for doc in comments_prep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff327ed2",
   "metadata": {},
   "source": [
    "# TFIDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef3e42",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc7d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_raw = df.comment.fillna('').tolist()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 0.01)\n",
    "#dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "#dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47632af9",
   "metadata": {},
   "source": [
    "### Training The Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3601ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topics(model, vectorizer, topn=5):\n",
    "    names = tf_vectorizer.get_feature_names()\n",
    "    res = {}\n",
    "    for i_cluster, cluster in enumerate(model.components_):\n",
    "        res[i_cluster]=[]\n",
    "        for i_feature in cluster.argsort()[:-topn - 1:-1]:\n",
    "            res[i_cluster] = res[i_cluster] + [names[i_feature]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bf797a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/traffic/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['school', 'thank', 'quot', 'like', 'high'], 1: ['cancer', 'colon', 'youtube', 'amp', 'literally'], 2: ['love', 'cancer', 'simone', 'colon', 'year'], 3: ['video', 'life', 'just', 'like', 'time']}\n"
     ]
    }
   ],
   "source": [
    "#first get topics for all comments (Full Dataset)\n",
    "docs_raw_full = df.comment.fillna('').tolist()\n",
    "dtm_tf_full = tf_vectorizer.fit_transform(docs_raw_full)\n",
    "# train model to find topics per influencer\n",
    "lda_tf_full = LatentDirichletAllocation(n_components=4, random_state=0)\n",
    "lda_tf_full.fit(dtm_tf_full)\n",
    "# extract most important topics\n",
    "topics_full =  get_top_topics(lda_tf_full, tf_vectorizer, topn=5)\n",
    "print(topics_full)\n",
    "\n",
    "X_test = tf_vectorizer.transform(df.loc[:,'comment'])\n",
    "\n",
    "doc_topic_dist_unnormalized_full = np.matrix(lda_tf_full.transform(X_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17305a91",
   "metadata": {},
   "source": [
    "res_ = pd.DataFrame(doc_topic_dist_unnormalized_full.argmax(axis=1)).value_counts().rename_axis('topic').reset_index(name='counts')\n",
    "counts_sum = res_.counts.sum()\n",
    "res_.loc[:,'counts'] = res_.counts.apply(lambda x:round(x/counts_sum*100,1))\n",
    "res_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44eb0231",
   "metadata": {},
   "source": [
    "pd.DataFrame(doc_topic_dist_unnormalized_full.argmax(axis=1)).value_counts().rename_axis('topic').reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ac90746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['school', 'thank', 'quot', 'like', 'high'], 1: ['cancer', 'colon', 'youtube', 'amp', 'literally'], 2: ['love', 'cancer', 'simone', 'colon', 'year'], 3: ['video', 'life', 'just', 'like', 'time']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/traffic/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic\n",
       "0      0\n",
       "1      1\n",
       "2      2\n",
       "3      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['itssozer', 59.2, 8.2, 14.3, 18.4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayyyy legend\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Are you serious\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Yes it's college but it's called university\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Your such an amazing person üòÅ\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "if i got chegg as a gift i would fucking die @Antedragz\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "That‚Äôs a different thing in my country tho\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "ÔºöCe gars est une legende üëè\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Omg your so nice\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "What a gift\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "HEAVEN NOW\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "I love your page @itssozer ü•∞üòÅ\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "Why would anyone ever hurt you? Your the nicest person I knowü•∞üòÅ\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "hii I have CRPS a very rare disease and my mum dose not have enough money to pay for my medical bills\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "Yes it‚Äôs university\n",
      "['cancer', 'colon', 'youtube', 'amp', 'literally']\n",
      "Why don‚Äôt you guys get some goods from the soulmia appü•∞\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "@Antedragz\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "The best\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "is this sponsored? idk what gave it awayüò≥\n",
      "['cancer', 'colon', 'youtube', 'amp', 'literally']\n",
      "me wathching her tiktok:üò≥..\n",
      "mom:why you sad\n",
      "me:because my brother he broke my cellphone...\n",
      "@itssozer this not making story this is true...\n",
      "['cancer', 'colon', 'youtube', 'amp', 'literally']\n",
      "uni means university and usually a uni will have lots of colleges under it that have different fields and majors you can study\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "this is the best acount i‚Äôve ever seen, it makes me be happy, you are the best broü•∞\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "sorry to educate myself\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "You are awesome nice kind and it makes me and everyone happy\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "This guy literally always makes my day\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "He‚Äôs in Adelaide\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "@jenna.michellle RAWR\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "üôå‚ù§Ô∏è\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Uni is so hard! so happy to see you're supporting some uni students\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "I‚Äôm just trying to educate him like he wanted ü§∑üèª‚Äç‚ôÄÔ∏è\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "Same here but I think this person is American\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "Lmao good for you? üòÇ\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "Okay so will I\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "You too üòÇ\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "I‚Äôm so glad I came across you ma‚Äôam @Aviana Dominic my financial debt is now a thing of the past. God bless you ma‚Äôam\n",
      "['cancer', 'colon', 'youtube', 'amp', 'literally']\n",
      "Your a very kind person ü•∞\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "No problem üòÅ\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Can I get a hi ü•∞ your my favourite and first to like!\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "OMG\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "Does the king reply? üëë\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "I‚Äôll do what i want thankssss\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Uni is stresssssfffffuuuuulllllllll\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "thanks\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Does the goat reply? Pls do it will make my day üòÖ\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "you so nice\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Not in the UK; college & Uni are different things\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Yeah I know but that person I think is from America and they call uni college\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "14th\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "1m and hi\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "what's UNI? College?\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "[0.7222222222222222, 0.006944444444444444, 0.24305555555555555, 0.027777777777777776, 104, 1, 35, 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic\n",
       "0      0\n",
       "1      1\n",
       "2      2\n",
       "3      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['emilyballz', 73.7, 5.3, 15.8, 5.3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "it's just to much effort and I hate school\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "I‚Äôve been following for a year! I‚Äôm making you a go now!\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "Hi\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Emily are you single?\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Ty for replying üòä\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Ya\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Hi,I am really interested in you I would like us to work something out ,am ready to give you 200bucks\n",
      "['video', 'life', 'just', 'like', 'time']\n",
      "Hi\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Damn\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Hi!\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Quizlet is also mad helpfully too and free\n",
      "['cancer', 'colon', 'youtube', 'amp', 'literally']\n",
      "I'm to lazy to do homework\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "True but not the same as Chegg:) Chegg can help teach you where quizlets is for flash cards and memorization\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "Chase a bag üíº\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "but i wanna see your wall\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "üòÇüòÇüòÇ I love you üòò\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "üòÇüòÇüòÇ I love you üòò\n",
      "['love', 'cancer', 'simone', 'colon', 'year']\n",
      "but i wanna see your wall\n",
      "['school', 'thank', 'quot', 'like', 'high']\n",
      "[0.7894736842105263, 0.15789473684210525, 0.0, 0.05263157894736842, 15, 3, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#first get topics for all comments (Full Dataset)\n",
    "docs_raw_full = df.comment.fillna('').tolist()\n",
    "dtm_tf_full = tf_vectorizer.fit_transform(docs_raw_full)\n",
    "# train model to find topics per influencer\n",
    "lda_tf_full = LatentDirichletAllocation(n_components=4, random_state=0)\n",
    "lda_tf_full.fit(dtm_tf_full)\n",
    "# extract most important topics\n",
    "topics_full =  get_top_topics(lda_tf_full, tf_vectorizer, topn=5)\n",
    "print(topics_full)\n",
    "\n",
    "#now let's what of those topics are talked about by what influencer\n",
    "# and then go more granular and look at topics on influencer level.\n",
    "topic_mapping = pd.DataFrame()\n",
    "res_infl = pd.DataFrame({'influencer':[]})\n",
    "for topic in topics_full:\n",
    "    topic_mapping.loc[:,topic] = np.array([str(topics_full[topic])])\n",
    "    res_infl[topic] = []\n",
    "    \n",
    "for topic in topics_full:\n",
    "    res_infl['likesrel_'+str(topic)] = []\n",
    "for topic in topics_full:\n",
    "    res_infl['likestot_'+str(topic)] = []\n",
    "\n",
    "for i_infl, infl in enumerate(df.influencer.unique()):\n",
    "    dfi = df.loc[df.influencer==infl,:]\n",
    "    #what categories of full topic model do infl comments belong to\n",
    "    X_test = tf_vectorizer.transform(dfi.loc[:,'comment'])\n",
    "    doc_topic_dist_unnormalized_full = np.matrix(lda_tf_full.transform(X_test))\n",
    "    # get count of number topics are 'hit'\n",
    "    res_ = pd.DataFrame({'topic':list(topics_full.keys())})\n",
    "    display(res_.head())\n",
    "    res__ = pd.DataFrame(doc_topic_dist_unnormalized_full.argmax(axis=1)).value_counts().rename_axis('topic').reset_index(name='counts')\n",
    "    res_ = res_.merge(res__, on='topic', how='left').reset_index(drop=True).fillna(0)\n",
    "    \n",
    "    # calculate relative topic distribution for influencer\n",
    "    counts_sum = res_.counts.sum()\n",
    "    res_.loc[:,'counts'] = res_.counts.apply(lambda x:round(x/counts_sum*100,1))\n",
    "    res_ = res_.sort_values('topic',ascending=True).reset_index(drop=True)\n",
    "    display([infl] + list(res_.counts))\n",
    "    \n",
    "    l_likecount_tot = len(topics_full)*[0]\n",
    "    #l_likecount_rel = len(topics_full)*[0]\n",
    "    total_likecount = dfi.comment_likecount.sum()\n",
    "    for comment, comment_orig, comment_likes in zip(doc_topic_dist_unnormalized_full, dfi.comment, dfi.comment_likecount):\n",
    "        print(comment_orig)\n",
    "        print(topics_full[comment[0].argmax()])\n",
    "        l_likecount_tot[comment[0].argmax()]+=comment_likes\n",
    "    l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
    "    print(l_likecount_rel + l_likecount_tot)\n",
    "    res_infl.loc[len(res_infl)] = [infl] + list(res_.counts) + l_likecount_rel + l_likecount_tot\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2fb106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/traffic/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['school', 'thank', 'quot', 'like', 'high'], 1: ['cancer', 'colon', 'youtube', 'amp', 'literally'], 2: ['love', 'cancer', 'simone', 'colon', 'year'], 3: ['video', 'life', 'just', 'like', 'time']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3707/1667404526.py:48: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_3707/1667404526.py:48: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_3707/1667404526.py:48: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_3707/1667404526.py:48: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_3707/1667404526.py:48: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
      "/tmp/ipykernel_3707/1667404526.py:48: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n"
     ]
    }
   ],
   "source": [
    "#first get topics for all comments (Full Dataset)\n",
    "docs_raw_full = df.comment.fillna('').tolist()\n",
    "dtm_tf_full = tf_vectorizer.fit_transform(docs_raw_full)\n",
    "# train model to find topics per influencer\n",
    "lda_tf_full = LatentDirichletAllocation(n_components=4, random_state=0)\n",
    "lda_tf_full.fit(dtm_tf_full)\n",
    "# extract most important topics\n",
    "topics_full =  get_top_topics(lda_tf_full, tf_vectorizer, topn=5)\n",
    "print(topics_full)\n",
    "\n",
    "#now let's what of those topics are talked about by what influencer\n",
    "# and then go more granular and look at topics on influencer level.\n",
    "topic_mapping = pd.DataFrame()\n",
    "res_infl = pd.DataFrame({'influencer':[]})\n",
    "for topic in topics_full:\n",
    "    topic_mapping.loc[:,topic] = np.array([str(topics_full[topic])])\n",
    "    res_infl[topic] = []\n",
    "    \n",
    "for topic in topics_full:\n",
    "    res_infl['likesrel_'+str(topic)] = []\n",
    "for topic in topics_full:\n",
    "    res_infl['likestot_'+str(topic)] = []\n",
    "\n",
    "for i_infl, infl in enumerate(df.influencer.unique()):\n",
    "    dfi = df.loc[df.influencer==infl,:]\n",
    "    #what categories of full topic model do infl comments belong to\n",
    "    X_test = tf_vectorizer.transform(dfi.loc[:,'comment'])\n",
    "    doc_topic_dist_unnormalized_full = np.matrix(lda_tf_full.transform(X_test))\n",
    "    # get count of number topics are 'hit'\n",
    "    res_ = pd.DataFrame({'topic':list(topics_full.keys())})\n",
    "    #display(res_.head())\n",
    "    res__ = pd.DataFrame(doc_topic_dist_unnormalized_full.argmax(axis=1)).value_counts().rename_axis('topic').reset_index(name='counts')\n",
    "    res_ = res_.merge(res__, on='topic', how='left').reset_index(drop=True).fillna(0)\n",
    "    \n",
    "    # calculate relative topic distribution for influencer\n",
    "    counts_sum = res_.counts.sum()\n",
    "    res_.loc[:,'counts'] = res_.counts.apply(lambda x:round(x/counts_sum*100,1))\n",
    "    res_ = res_.sort_values('topic',ascending=True).reset_index(drop=True)\n",
    "    #display([infl] + list(res_.counts))\n",
    "    \n",
    "    l_likecount_tot = len(topics_full)*[0]\n",
    "    #l_likecount_rel = len(topics_full)*[0]\n",
    "    total_likecount = dfi.comment_likecount.sum()\n",
    "    for comment, comment_orig, comment_likes in zip(doc_topic_dist_unnormalized_full, dfi.comment, dfi.comment_likecount):\n",
    "        #print(comment_orig)\n",
    "        #print(topics_full[comment[0].argmax()])\n",
    "        l_likecount_tot[comment[0].argmax()]+=comment_likes\n",
    "    l_likecount_rel = [v/total_likecount for v in l_likecount_tot]\n",
    "\n",
    "    res_infl.loc[len(res_infl)] = [infl] + list(res_.counts) + l_likecount_rel + l_likecount_tot\n",
    "        \n",
    "        \n",
    "    \"\"\"  \n",
    "    docs_raw = dfi.comment.fillna('').tolist()\n",
    "    dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
    "    # train model to find topics per influencer\n",
    "    lda_tf = LatentDirichletAllocation(n_components=4, random_state=0)\n",
    "    lda_tf.fit(dtm_tf)d\n",
    "    # extract most important topics\n",
    "    topics =  get_top_topics(lda_tf, tf_vectorizer, topn=5)\n",
    "    print(infl)\n",
    "    print(topics)\n",
    "    \n",
    "    doc_topic_dist_unnormalized = np.matrix(lda_tf.transform(X_test))\n",
    "    for comment, comment_orig in zip(doc_topic_dist_unnormalized, dfi.comment):\n",
    "        print(comment_orig)\n",
    "        print(topics[comment[0].argmax()])\n",
    "    \"\"\"\n",
    "    #if i_infl>-1:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59bea655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>influencer</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>likesrel_0</th>\n",
       "      <th>likesrel_1</th>\n",
       "      <th>likesrel_2</th>\n",
       "      <th>likesrel_3</th>\n",
       "      <th>likestot_0</th>\n",
       "      <th>likestot_1</th>\n",
       "      <th>likestot_2</th>\n",
       "      <th>likestot_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>itssozer</td>\n",
       "      <td>59.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emilyballz</td>\n",
       "      <td>73.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iamalilstitious</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sarahbada_</td>\n",
       "      <td>73.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sakshammagic</td>\n",
       "      <td>73.1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        influencer     0     1     2     3  likesrel_0  likesrel_1  \\\n",
       "0         itssozer  59.2   8.2  14.3  18.4    0.722222    0.006944   \n",
       "1       emilyballz  73.7   5.3  15.8   5.3    0.789474    0.157895   \n",
       "2  iamalilstitious  50.0   0.0  37.5  12.5    0.666667    0.000000   \n",
       "3       sarahbada_  73.7   5.3  15.8   5.3    0.913043    0.000000   \n",
       "4     sakshammagic  73.1  11.5   3.8  11.5    0.761905    0.142857   \n",
       "\n",
       "   likesrel_2  likesrel_3  likestot_0  likestot_1  likestot_2  likestot_3  \n",
       "0    0.243056    0.027778         104           1          35           4  \n",
       "1    0.000000    0.052632          15           3           0           1  \n",
       "2    0.333333    0.000000           2           0           1           0  \n",
       "3    0.086957    0.000000          21           0           2           0  \n",
       "4    0.000000    0.095238          16           3           0           2  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_infl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "669c4575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table datascience-abovezero.ml_sandbox.chegg_influencers_comments_results already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/traffic/.local/lib/python3.8/site-packages/pandas/io/gbq.py:212: DeprecationWarning: chunksize is ignored when using api_method='load_parquet'\n",
      "  pandas_gbq.to_gbq(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 9467.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_results(df, table_id=\"datascience-abovezero.ml_sandbox.chegg_influencers_comments_results\"):\n",
    "    # create BigQuery table if it doesn't already exist\n",
    "    try: \n",
    "        client.get_table(table_id)  # Make an API request.\n",
    "        print(\"Table {} already exists.\".format(table_id))\n",
    "    except NotFound:\n",
    "        print(f\"Table {table_id} created.\")\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"influencer\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('_'.join(['topic0'] + topics_full[0]), \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('_'.join(['topic1'] + topics_full[1]), \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('_'.join(['topic2'] + topics_full[2]), \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('_'.join(['topic3'] + topics_full[3]), \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likesrel0', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likesrel1', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likesrel2', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likesrel3', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likestot0', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likestot1', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likestot2', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField('likestot3', \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "        ]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        #print(\n",
    "        #    \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        #)\n",
    "    df.columns = ['influencer', '_'.join(['topic']+topics_full[0]),'_'.join(['topic']+topics_full[1]),'_'.join(['topic']+topics_full[2]),'_'.join(['topic']+topics_full[3]),\n",
    "                 'likesrel0','likesrel1','likesrel2','likesrel3','likestot0','likestot1','likestot2','likestot3']     \n",
    "    df.to_gbq('.'.join(table_id.split('.')[1:]), project_id=table_id.split('.')[0], if_exists='replace',#'append',\n",
    "          chunksize=10000, progress_bar=True, credentials=credentials)\n",
    "    \n",
    "    \n",
    "save_results(res_infl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63df5e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>influencer</th>\n",
       "      <th>topic_school_thank_quot_like_high</th>\n",
       "      <th>topic_cancer_colon_youtube_amp_literally</th>\n",
       "      <th>topic_love_cancer_simone_colon_year</th>\n",
       "      <th>topic_video_life_just_like_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>itssozer</td>\n",
       "      <td>59.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emilyballz</td>\n",
       "      <td>73.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iamalilstitious</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sarahbada_</td>\n",
       "      <td>73.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sakshammagic</td>\n",
       "      <td>73.1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        influencer  topic_school_thank_quot_like_high  \\\n",
       "0         itssozer                               59.2   \n",
       "1       emilyballz                               73.7   \n",
       "2  iamalilstitious                               50.0   \n",
       "3       sarahbada_                               73.7   \n",
       "4     sakshammagic                               73.1   \n",
       "\n",
       "   topic_cancer_colon_youtube_amp_literally  \\\n",
       "0                                       8.2   \n",
       "1                                       5.3   \n",
       "2                                       0.0   \n",
       "3                                       5.3   \n",
       "4                                      11.5   \n",
       "\n",
       "   topic_love_cancer_simone_colon_year  topic_video_life_just_like_time  \n",
       "0                                 14.3                             18.4  \n",
       "1                                 15.8                              5.3  \n",
       "2                                 37.5                             12.5  \n",
       "3                                 15.8                              5.3  \n",
       "4                                  3.8                             11.5  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_infl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f839961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
