{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# So, What is TF-IDF?\n*(if you're not interested in the theory part of TF-IDF, feel free to skip to the code section.)*\n![](https://monkeylearn.com/static/679ad6824cd3f362d6081c38b8ef5824/35d2d/What-is-TF-IDF-Normal.png)\n\n### ***Okay, theory time!***\n\n**TF-IDF** is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n\nIt has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP).\n\nTF-IDF (**term frequency-inverse document frequency**) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular.\n\nHowever, if the word Bug appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant. For example, if what we’re doing is trying to find out which topics some NPS responses belong to, the word Bug would probably end up being tied to the topic Reliability, since most responses containing that word would be about that topic.\n\n### ***Math behind TF-IDF...***\n**TF-IDF** for a word in a document is calculated by multiplying two different metrics:\n\n- The **term frequency** of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n- The **inverse document frequency** of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n\nSo, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\nMultiplying these two numbers results in the **TF-IDF score** of a word in a document. The higher the score, the more relevant that word is in that particular document.\n\nTo put it in more formal mathematical terms, the TF-IDF score for the word `t` in the document `d` from the document set `D` is calculated as follows:\n\n![](https://monkeylearn.com/static/23b5e36265d19e9b42a9ae42220d257b/df264/1.png)\n\nWhere:\n\n![](https://monkeylearn.com/static/d96cda57105351e7b75b844910ab3f73/df264/2.png)\n\n![](https://monkeylearn.com/static/aaa7bf8149587b9b828f99e1db9f7e46/df264/3.png)\n\n*Source:* [*https://monkeylearn.com/blog/what-is-tf-idf/*](https://monkeylearn.com/blog/what-is-tf-idf/)","metadata":{}},{"cell_type":"markdown","source":"**Without further ado, let's dive into the practical use-case of TF-IDF.**\n\n**We'll be extracting the top 10 keywords from the published papers in [NeurIPS Conferences](https://www.kaggle.com/rowhitswami/nips-papers-1987-2019-updated/) from 1987 to 2019 using built-in algorithm of TD-IDF in Scikit-learn library.**","metadata":{}},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"# Printing data files\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('../input/facebook/round_23.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T20:50:02.571030Z","iopub.execute_input":"2022-02-03T20:50:02.571568Z","iopub.status.idle":"2022-02-03T20:50:02.585497Z","shell.execute_reply.started":"2022-02-03T20:50:02.571531Z","shell.execute_reply":"2022-02-03T20:50:02.584143Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# General libraries\nimport re, os, string\nimport pandas as pd\n\n# Scikit-learn importings\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:02.588119Z","iopub.execute_input":"2022-02-03T20:50:02.588515Z","iopub.status.idle":"2022-02-03T20:50:03.898318Z","shell.execute_reply.started":"2022-02-03T20:50:02.588482Z","shell.execute_reply":"2022-02-03T20:50:03.896821Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_stopwords_list(stop_file_path):\n    \"\"\"load stop words \"\"\"\n    \n    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n        stopwords = f.readlines()\n        stop_set = set(m.strip() for m in stopwords)\n        return list(frozenset(stop_set))","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-02-03T20:50:03.900501Z","iopub.execute_input":"2022-02-03T20:50:03.901005Z","iopub.status.idle":"2022-02-03T20:50:03.908795Z","shell.execute_reply.started":"2022-02-03T20:50:03.900950Z","shell.execute_reply":"2022-02-03T20:50:03.907398Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    \"\"\"Doc cleaning\"\"\"\n    \n    # Lowering text\n    text = text.lower()\n    \n    # Removing punctuation\n    text = \"\".join([c for c in text if c not in PUNCTUATION])\n    \n    # Removing whitespace and newlines\n    text = re.sub('\\s+',' ',text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:03.910700Z","iopub.execute_input":"2022-02-03T20:50:03.911202Z","iopub.status.idle":"2022-02-03T20:50:03.925476Z","shell.execute_reply.started":"2022-02-03T20:50:03.911156Z","shell.execute_reply":"2022-02-03T20:50:03.924201Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def sort_coo(coo_matrix):\n    \"\"\"Sort a dict with highest score\"\"\"\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature, score\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:03.930422Z","iopub.execute_input":"2022-02-03T20:50:03.930920Z","iopub.status.idle":"2022-02-03T20:50:03.944825Z","shell.execute_reply.started":"2022-02-03T20:50:03.930874Z","shell.execute_reply":"2022-02-03T20:50:03.943636Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_keywords(vectorizer, feature_names, doc):\n    \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n\n    #generate tf-idf for the given document\n    tf_idf_vector = vectorizer.transform([doc])\n    \n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tf_idf_vector.tocoo())\n\n    #extract only TOP_K_KEYWORDS\n    keywords=extract_topn_from_vector(feature_names,sorted_items,TOP_K_KEYWORDS)\n    \n    return list(keywords.keys())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:03.947086Z","iopub.execute_input":"2022-02-03T20:50:03.947506Z","iopub.status.idle":"2022-02-03T20:50:03.960212Z","shell.execute_reply.started":"2022-02-03T20:50:03.947467Z","shell.execute_reply":"2022-02-03T20:50:03.958676Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Constants\nPUNCTUATION = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\" \nTOP_K_KEYWORDS = 10 # top k number of keywords to retrieve in a ranked document\nSTOPWORD_PATH = \"/kaggle/input/stopwords/stopwords.txt\"\nPAPERS_PATH = \"/kaggle/input/nips-papers-1987-2019-updated/papers.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:03.962167Z","iopub.execute_input":"2022-02-03T20:50:03.962591Z","iopub.status.idle":"2022-02-03T20:50:03.981922Z","shell.execute_reply.started":"2022-02-03T20:50:03.962543Z","shell.execute_reply":"2022-02-03T20:50:03.980926Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Reading data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/facebook/round_23.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:03.984052Z","iopub.execute_input":"2022-02-03T20:50:03.984558Z","iopub.status.idle":"2022-02-03T20:50:04.685809Z","shell.execute_reply.started":"2022-02-03T20:50:03.984508Z","shell.execute_reply":"2022-02-03T20:50:04.684641Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data.dropna(subset=['text'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:04.687517Z","iopub.execute_input":"2022-02-03T20:50:04.688087Z","iopub.status.idle":"2022-02-03T20:50:04.757207Z","shell.execute_reply.started":"2022-02-03T20:50:04.688034Z","shell.execute_reply":"2022-02-03T20:50:04.755603Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Preparing data","metadata":{}},{"cell_type":"code","source":"data['text'] = data['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:04.759230Z","iopub.execute_input":"2022-02-03T20:50:04.759718Z","iopub.status.idle":"2022-02-03T20:50:07.611113Z","shell.execute_reply.started":"2022-02-03T20:50:04.759669Z","shell.execute_reply":"2022-02-03T20:50:07.609576Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:07.613259Z","iopub.execute_input":"2022-02-03T20:50:07.613666Z","iopub.status.idle":"2022-02-03T20:50:07.632780Z","shell.execute_reply.started":"2022-02-03T20:50:07.613630Z","shell.execute_reply":"2022-02-03T20:50:07.631411Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"corpora = data['text'].to_list()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:07.634427Z","iopub.execute_input":"2022-02-03T20:50:07.635040Z","iopub.status.idle":"2022-02-03T20:50:07.656644Z","shell.execute_reply.started":"2022-02-03T20:50:07.634965Z","shell.execute_reply":"2022-02-03T20:50:07.655207Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Keywords Extraction using TF-IDF","metadata":{}},{"cell_type":"code","source":"#load a set of stop words\nstopwords=get_stopwords_list(STOPWORD_PATH)\n\n# Initializing TF-IDF Vectorizer with stopwords\nvectorizer = TfidfVectorizer(stop_words=stopwords, smooth_idf=True, use_idf=True)\n\n# Creating vocab with our corpora\n# Exlcluding first 10 docs for testing purpose\nvectorizer.fit_transform(corpora[10::])\n\n# Storing vocab\nfeature_names = vectorizer.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:07.658567Z","iopub.execute_input":"2022-02-03T20:50:07.659221Z","iopub.status.idle":"2022-02-03T20:50:11.210092Z","shell.execute_reply.started":"2022-02-03T20:50:07.659161Z","shell.execute_reply":"2022-02-03T20:50:11.208477Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Result \n","metadata":{}},{"cell_type":"code","source":"result = []\nfor doc in corpora[0:50]:\n    df = {}\n    df['text'] = doc\n    df['top_keywords'] = get_keywords(vectorizer, feature_names, doc)\n    result.append(df)\n    \nfinal = pd.DataFrame(result)\nfinal","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:11.211547Z","iopub.execute_input":"2022-02-03T20:50:11.212030Z","iopub.status.idle":"2022-02-03T20:50:11.339540Z","shell.execute_reply.started":"2022-02-03T20:50:11.211940Z","shell.execute_reply":"2022-02-03T20:50:11.338358Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pip install cufflinks","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:11.341198Z","iopub.execute_input":"2022-02-03T20:50:11.342005Z","iopub.status.idle":"2022-02-03T20:50:42.626705Z","shell.execute_reply.started":"2022-02-03T20:50:11.341743Z","shell.execute_reply":"2022-02-03T20:50:42.625406Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from plotly.offline import init_notebook_mode,iplot\nimport plotly.graph_objects as go\nimport cufflinks as cf\n# init_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:42.629655Z","iopub.execute_input":"2022-02-03T20:50:42.630457Z","iopub.status.idle":"2022-02-03T20:50:45.688097Z","shell.execute_reply.started":"2022-02-03T20:50:42.630395Z","shell.execute_reply":"2022-02-03T20:50:45.686592Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:50:45.690091Z","iopub.execute_input":"2022-02-03T20:50:45.690505Z","iopub.status.idle":"2022-02-03T20:50:45.709270Z","shell.execute_reply.started":"2022-02-03T20:50:45.690468Z","shell.execute_reply":"2022-02-03T20:50:45.708051Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from plotly.offline import init_notebook_mode,iplot\nimport plotly.graph_objects as go\nimport cufflinks as cf\ninit_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:51:20.265087Z","iopub.execute_input":"2022-02-03T20:51:20.265515Z","iopub.status.idle":"2022-02-03T20:51:20.342631Z","shell.execute_reply.started":"2022-02-03T20:51:20.265479Z","shell.execute_reply":"2022-02-03T20:51:20.341658Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"texto_de_publicaciones = df_shares['co']\ntexto_de_publicaciones = [i for i in texto_de_publicaciones if type(i) == str]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}