{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J2XzALU6mQjC"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sentiment as sent\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 1\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 32\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 60\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuraiton...\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cpu`\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56015266d1dc47beb8d328c958d8234f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe80c993e4b54d98a69d261042df8845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 25000 examples!\n",
      "Created `train_dataloader` with 782 batches!\n",
      "\n",
      "Dealing with Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b316620fab994046b3f75e3441f7d7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161895c25ec148f7b7e4f6b72ed150e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `valid_dataset` with 25000 examples!\n",
      "Created `eval_dataloader` with 782 batches!\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = sent.Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = sent.MovieReviewsDataset(path='/home/traffic/data/aclImdb/train', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = sent.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  sent.MovieReviewsDataset(path='/home/traffic/data/aclImdb/test', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = sent.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86253185fd034daf9c2e47d7017d6b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ee96f9f9284c24a3148d5e07681f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc732e164fe42deaf2234d8325e187a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
    "# us the number of batches.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print()\n",
    "  print('Training on batches...')\n",
    "  # Perform one full pass over the training set.\n",
    "  train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "  train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "  # Get prediction form model on validation data. \n",
    "  print('Validation on batches...')\n",
    "  valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "  val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "  # Print loss and accuracy values to see how training evolves.\n",
    "  print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "  print()\n",
    "\n",
    "  # Store the loss value for plotting the learning curve.\n",
    "  all_loss['train_loss'].append(train_loss)\n",
    "  all_loss['val_loss'].append(val_loss)\n",
    "  all_acc['train_acc'].append(train_acc)\n",
    "  all_acc['val_acc'].append(val_acc)\n",
    "\n",
    "# Plot loss curves.\n",
    "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
    "\n",
    "# Plot accuracy curves.\n",
    "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction form model on validation data. This is where you should use\n",
    "# your test data.\n",
    "true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)\n",
    "\n",
    "# Create the evaluation report.\n",
    "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
    "# Show the evaluation report.\n",
    "print(evaluation_report)\n",
    "\n",
    "# Plot confusion matrix.\n",
    "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels, \n",
    "                      classes=list(labels_ids.keys()), normalize=True, \n",
    "                      magnify=0.1,\n",
    "                      );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPezIEYymQjD"
   },
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ObOL1qx5mQjF"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('analisis_comments_tiktok.csv')\n",
    "comments = [str(comment) for comment in df.comment.fillna('').tolist()]\n",
    "chegg_dataset = CheggDataset(df=comments, use_tokenizer=tokenizer)\n",
    "\n",
    "chegg_dataloader = DataLoader(chegg_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ac2e84728f409e8fa13fa8febd77e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, predictions_labels, _ = validation(chegg_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it’s not good chegg apparently rats out on stu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I hope Audri looks different at that age</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I absolutely love it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>AYYY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>guys there is a 100% free app called oh my hom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>WHY THERE NO GIRLS OML😂</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>Really good teacher🥰! love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>what's UNI? College?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comments  labels\n",
       "0                                                             0\n",
       "1     it’s not good chegg apparently rats out on stu...       0\n",
       "2                                                             0\n",
       "3              I hope Audri looks different at that age       0\n",
       "4                                 I absolutely love it!       1\n",
       "...                                                 ...     ...\n",
       "1091                                               AYYY       0\n",
       "1092  guys there is a 100% free app called oh my hom...       1\n",
       "1093                            WHY THERE NO GIRLS OML😂       0\n",
       "1094                         Really good teacher🥰! love       1\n",
       "1095                               what's UNI? College?       0\n",
       "\n",
       "[1096 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'comments':comments,'labels':predictions_labels})\n",
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "comment analysis.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "86a2e1f725dc4ce1f0c9e9b47640d49ab83eaba388420931ab91f013b3e264c3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
